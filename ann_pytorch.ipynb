{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf01765b-c84a-414a-a0b6-89f0808dafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8eb31f6-a222-46ae-9a5a-04fbbb5f00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'chest_xray/train'\n",
    "test_dir = 'chest_xray/test'\n",
    "val_dir = 'chest_xray/val'\n",
    "\n",
    "# Image dimensions and batch size\n",
    "image_size = (224, 224)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab9f0ba-5b3c-45d5-bc8d-293b1d48ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181b0434-6bfa-49ad-a7a2-97ece72b13ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f78f91-8b40-49c0-946f-792d117b47d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7298bf12-328e-4f25-a7d3-49ec665dc75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0819b85b-4f52-4431-b197-5a72e1f95a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_generator):\n",
    "        self.image_generator = image_generator\n",
    "        self.data = image_generator[0][0]  # Extract images\n",
    "        self.labels = image_generator[0][1]  # Extract labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert image data to PyTorch tensor\n",
    "        image = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# Assuming you have an ImageDataGenerator (train_generator)\n",
    "train_dataset = ImageDataset(train_generator)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "333103ef-ad48-40de-97d2-9941e8e30a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)  # Input layer\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        #self.fc3 = nn.Linear(256, 256)\n",
    "        #self.fc4 = nn.Linear(256, 256)\n",
    "        #self.fc5 = nn.Linear(256, 256)\n",
    "        self.fc6 = nn.Linear(256, 256)\n",
    "        self.fc7 = nn.Linear(256, 1)           # Output layer for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()           # Sigmoid activation for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)  # Flatten image to 1D (for ANN)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #x = torch.relu(self.fc3(x))\n",
    "        #x = torch.relu(self.fc4(x))\n",
    "        #x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = self.sigmoid(self.fc7(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e7fc31-71f2-48fb-8c9a-ed5824233247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6897\n",
      "Epoch [2/10], Loss: 0.4755\n",
      "Epoch [3/10], Loss: 0.4590\n",
      "Epoch [4/10], Loss: 0.4413\n",
      "Epoch [5/10], Loss: 0.4429\n",
      "Epoch [6/10], Loss: 0.3972\n",
      "Epoch [7/10], Loss: 0.3976\n",
      "Epoch [8/10], Loss: 0.3619\n",
      "Epoch [9/10], Loss: 0.3516\n",
      "Epoch [10/10], Loss: 0.3093\n",
      "Epoch [11/10], Loss: 0.3101\n",
      "Epoch [12/10], Loss: 0.2677\n",
      "Epoch [13/10], Loss: 0.2637\n",
      "Epoch [14/10], Loss: 0.2300\n",
      "Epoch [15/10], Loss: 0.2186\n",
      "Epoch [16/10], Loss: 0.1944\n",
      "Epoch [17/10], Loss: 0.1787\n",
      "Epoch [18/10], Loss: 0.1649\n",
      "Epoch [19/10], Loss: 0.1439\n",
      "Epoch [20/10], Loss: 0.1366\n",
      "Epoch [21/10], Loss: 0.1170\n",
      "Epoch [22/10], Loss: 0.1096\n",
      "Epoch [23/10], Loss: 0.0962\n",
      "Epoch [24/10], Loss: 0.0854\n",
      "Epoch [25/10], Loss: 0.0787\n",
      "Epoch [26/10], Loss: 0.0671\n",
      "Epoch [27/10], Loss: 0.0618\n",
      "Epoch [28/10], Loss: 0.0550\n",
      "Epoch [29/10], Loss: 0.0476\n",
      "Epoch [30/10], Loss: 0.0439\n",
      "Epoch [31/10], Loss: 0.0386\n",
      "Epoch [32/10], Loss: 0.0336\n",
      "Epoch [33/10], Loss: 0.0309\n",
      "Epoch [34/10], Loss: 0.0274\n",
      "Epoch [35/10], Loss: 0.0239\n",
      "Epoch [36/10], Loss: 0.0218\n",
      "Epoch [37/10], Loss: 0.0198\n",
      "Epoch [38/10], Loss: 0.0175\n",
      "Epoch [39/10], Loss: 0.0156\n",
      "Epoch [40/10], Loss: 0.0143\n",
      "Epoch [41/10], Loss: 0.0130\n",
      "Epoch [42/10], Loss: 0.0116\n",
      "Epoch [43/10], Loss: 0.0104\n",
      "Epoch [44/10], Loss: 0.0095\n",
      "Epoch [45/10], Loss: 0.0088\n",
      "Epoch [46/10], Loss: 0.0080\n",
      "Epoch [47/10], Loss: 0.0072\n",
      "Epoch [48/10], Loss: 0.0067\n",
      "Epoch [49/10], Loss: 0.0062\n",
      "Epoch [50/10], Loss: 0.0058\n",
      "Epoch [51/10], Loss: 0.0053\n",
      "Epoch [52/10], Loss: 0.0049\n",
      "Epoch [53/10], Loss: 0.0045\n",
      "Epoch [54/10], Loss: 0.0042\n",
      "Epoch [55/10], Loss: 0.0040\n",
      "Epoch [56/10], Loss: 0.0037\n",
      "Epoch [57/10], Loss: 0.0035\n",
      "Epoch [58/10], Loss: 0.0033\n",
      "Epoch [59/10], Loss: 0.0031\n",
      "Epoch [60/10], Loss: 0.0030\n",
      "Epoch [61/10], Loss: 0.0028\n",
      "Epoch [62/10], Loss: 0.0027\n",
      "Epoch [63/10], Loss: 0.0026\n",
      "Epoch [64/10], Loss: 0.0025\n",
      "Epoch [65/10], Loss: 0.0023\n",
      "Epoch [66/10], Loss: 0.0022\n",
      "Epoch [67/10], Loss: 0.0022\n",
      "Epoch [68/10], Loss: 0.0021\n",
      "Epoch [69/10], Loss: 0.0020\n",
      "Epoch [70/10], Loss: 0.0019\n",
      "Epoch [71/10], Loss: 0.0018\n",
      "Epoch [72/10], Loss: 0.0018\n",
      "Epoch [73/10], Loss: 0.0017\n",
      "Epoch [74/10], Loss: 0.0017\n",
      "Epoch [75/10], Loss: 0.0016\n",
      "Epoch [76/10], Loss: 0.0016\n",
      "Epoch [77/10], Loss: 0.0015\n",
      "Epoch [78/10], Loss: 0.0015\n",
      "Epoch [79/10], Loss: 0.0014\n",
      "Epoch [80/10], Loss: 0.0014\n",
      "Epoch [81/10], Loss: 0.0013\n",
      "Epoch [82/10], Loss: 0.0013\n",
      "Epoch [83/10], Loss: 0.0012\n",
      "Epoch [84/10], Loss: 0.0012\n",
      "Epoch [85/10], Loss: 0.0012\n",
      "Epoch [86/10], Loss: 0.0011\n",
      "Epoch [87/10], Loss: 0.0011\n",
      "Epoch [88/10], Loss: 0.0011\n",
      "Epoch [89/10], Loss: 0.0010\n",
      "Epoch [90/10], Loss: 0.0010\n",
      "Epoch [91/10], Loss: 0.0010\n",
      "Epoch [92/10], Loss: 0.0009\n",
      "Epoch [93/10], Loss: 0.0009\n",
      "Epoch [94/10], Loss: 0.0009\n",
      "Epoch [95/10], Loss: 0.0009\n",
      "Epoch [96/10], Loss: 0.0009\n",
      "Epoch [97/10], Loss: 0.0008\n",
      "Epoch [98/10], Loss: 0.0008\n",
      "Epoch [99/10], Loss: 0.0008\n",
      "Epoch [100/10], Loss: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "model = ANNModel(input_size=224*224*3)  # Assuming images are 224x224\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4e4c4-320f-4eb4-a9d7-e78b8fc3599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader,\n",
    "epochs=20, device=\"cpu\"):\n",
    "for epoch in range(epochs):\n",
    "training_loss = 0.0\n",
    "valid_loss = 0.0\n",
    "model.train()\n",
    "for batch in train_loader:\n",
    "optimizer.zero_grad()\n",
    "inputs, target = batch\n",
    "inputs = inputs.to(device)\n",
    "target = targets.to(device)\n",
    "output = model(inputs)\n",
    "loss = loss_fn(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "training_loss += loss.data.item()\n",
    "training_loss /= len(train_iterator)\n",
    "model.eval()\n",
    "num_correct = 0\n",
    "num_examples = 0\n",
    "for batch in val_loader:\n",
    "inputs, targets = batch\n",
    "inputs = inputs.to(device)\n",
    "output = model(inputs)\n",
    "targets = targets.to(device)\n",
    "loss = loss_fn(output,targets)\n",
    "valid_loss += loss.data.item()\n",
    "correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],\n",
    "target).view(-1)\n",
    "num_correct += torch.sum(correct).item()\n",
    "num_examples += correct.shape[0]\n",
    "valid_loss /= len(valid_iterator)\n",
    "print('Epoch: {}, Training Loss: {:.2f},\n",
    "Validation Loss: {:.2f},\n",
    "accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a0f16d4-075e-4950-a53f-d072c788b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in test_loader:\n",
    "            # Move data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).float()  # Threshold at 0.5 for binary classification\n",
    "\n",
    "            # Calculate total and correct predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100  # Accuracy as a percentage\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "587533d3-c420-49ec-a7ee-6bfe3ee5842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset(test_generator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53dbaf83-201e-4053-aa73-a27455d6e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.38%\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f3779-7a53-40c8-b765-9caed387dc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
